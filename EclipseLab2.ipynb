{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJgWn0MHFKM+QpCYtplhdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qcritt/cosc470s24/blob/main/EclipseLab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Video\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "def extract_frames(video_path, output_dir):\n",
        "    # Make sure output directory exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Open the video file\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    success, image = vidcap.read()\n",
        "    count = 0\n",
        "\n",
        "    while success:\n",
        "        # Save frame as JPEG file\n",
        "        cv2.imwrite(os.path.join(output_dir, f\"frame{count:04d}.jpg\"), image)\n",
        "\n",
        "        success, image = vidcap.read()\n",
        "        print(f'Read a new frame: {success}', end='\\r')\n",
        "        count += 1\n",
        "\n",
        "# Example usage\n",
        "video_path = '/Users/quincycrittendon/Desktop/ResizedFrames/AdobeStock_191321618_Video_HD_Preview (1).mov'\n",
        "output_dir = '/Users/quincycrittendon/Desktop/ResizedFrames'\n",
        "\n",
        "extract_frames(video_path, output_dir)"
      ],
      "metadata": {
        "id": "ZgC22mpwLf2Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Resize\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def resize_images(directory):\n",
        "    # Get a list of all image files in the directory\n",
        "    image_files = [f for f in os.listdir(directory) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "    for file in image_files:\n",
        "        # Open the image file\n",
        "        image = Image.open(os.path.join(directory, file))\n",
        "\n",
        "        # Calculate the dimensions for cropping\n",
        "        width, height = image.size\n",
        "        if width > height:\n",
        "            left = (width - height) // 2\n",
        "            right = left + height\n",
        "            top = 0\n",
        "            bottom = height\n",
        "        else:\n",
        "            top = (height - width) // 2\n",
        "            bottom = top + width\n",
        "            left = 0\n",
        "            right = width\n",
        "\n",
        "        # Crop the image to the middle\n",
        "        image = image.crop((left, top, right, bottom))\n",
        "\n",
        "        # Resize the image to 32x32 pixels\n",
        "        image = image.resize((32, 32))\n",
        "\n",
        "        # Save the resized image\n",
        "        image.save(os.path.join(directory, f'resized_{file}'))\n",
        "\n",
        "    print('Image resizing complete.')\n",
        "\n",
        "# Specify the directory containing the images\n",
        "directory = '/Users/quincycrittendon/Desktop/ResizedFrames'\n",
        "\n",
        "# Call the resize_images function\n",
        "resize_images(directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySUyl-xILq6n",
        "outputId": "47cf78ff-eefb-4126-cc44-c4c8c1a28666"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image resizing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def load_custom_images(directory, size=(32,32)):\n",
        "    images = []\n",
        "    # List all files in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        # Construct the full path to the image\n",
        "        path = os.path.join(directory, filename)\n",
        "        # Open the image file\n",
        "        image = Image.open(path)\n",
        "        # Resize image (just in case some are not 32x32)\n",
        "        image = image.resize(size)\n",
        "        # Convert image to numpy array\n",
        "        image_data = np.asarray(image)\n",
        "        # Scale data to the range [-1, 1]\n",
        "        image_data = (image_data - 127.5) / 127.5\n",
        "        images.append(image_data)\n",
        "    # Convert list to a numpy array\n",
        "    images = np.array(images)\n",
        "    return images"
      ],
      "metadata": {
        "id": "jahkIo7wSRp8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZPKMfM3T7LH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def load_images_as_dataset(directory, size=(32,32), batch_size=32, shuffle_buffer_size=1000):\n",
        "    global total_num_images\n",
        "    global images\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.startswith(\"resized\") and filename.endswith(\".jpg\"):\n",
        "            img_path = os.path.join(directory, filename)\n",
        "            img = load_img(img_path, target_size=size)\n",
        "            img = img_to_array(img)\n",
        "            img = (img - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "            images.append(img)\n",
        "    total_num_images = len(images)\n",
        "    images = np.array(images)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(images).shuffle(shuffle_buffer_size).batch(batch_size, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "# Directory containing your images\n",
        "directory ='/Users/quincycrittendon/Desktop/ResizedFrames'\n",
        "batch_size = 32\n",
        "total_num_images = 0 # will be updated by load func\n",
        "dataset = load_images_as_dataset(directory, batch_size=batch_size)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU\n",
        "\n",
        "def make_generator_model():\n",
        "    model = Sequential([\n",
        "        Dense(8*8*256, use_bias=False, input_shape=(100,)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        Reshape((8, 8, 256)),\n",
        "        Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[32, 32, 3]),\n",
        "        LeakyReLU(),\n",
        "        Flatten(),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def train_step(images):\n",
        "    print(\"Shape of images before reshaping:\", images.shape)  # Debug statement\n",
        "\n",
        "    # Ensure the input has the correct shape\n",
        "    images = tf.reshape(images, (-1, 32, 32, 3))  # -1 is used to automatically calculate the needed batch size\n",
        "\n",
        "    print(\"Shape of images after reshaping:\", images.shape)\n",
        "\n",
        "\n",
        "def train_step(images):\n",
        "    # Ensure the input has the correct shape\n",
        "    images = tf.reshape(images, (-1, 32, 32, 3))  # -1 is used to automatically calculate the needed batch size\n",
        "\n",
        "    noise = tf.random.normal([len(images), 100])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "\n",
        "def train(dataset, epochs, batch_size):\n",
        "    global total_num_images\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch} starting ...\")\n",
        "        for batchi, image_batch in enumerate(dataset):\n",
        "            if batchi % 10 == 0:\n",
        "                print(f\"Batch {batchi}...\")\n",
        "            train_step(image_batch)\n",
        "        print(\"complete!\")\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(images).shuffle(tf.data.experimental.UNKNOWN_CARDINALITY).batch(batch_size, drop_remainder=True)\n",
        "        if epoch % 5 == 0:\n",
        "            show_gan(10, epoch)\n",
        "\n",
        "\n",
        "def show_gan(num_images, cnt):\n",
        "    # Generate images from the noise vector\n",
        "    noise = tf.random.normal([num_images, 100])\n",
        "    generated_images = generator(noise, training=False)\n",
        "\n",
        "    # Adjusting the pixel values to display them properly\n",
        "    generated_images = (generated_images + 1) / 2  # rescale from [-1, 1] to [0, 1]\n",
        "    generated_images = generated_images.numpy()  # convert to numpy array if not already\n",
        "\n",
        "    # Create a plot to display the images\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(20, 2))\n",
        "    for i, img in enumerate(generated_images):\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis('off')  # Turn off axis labels\n",
        "    fig.savefig(f\"generated{cnt}.png\", dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "train(dataset, 200, batch_size)  # Train for 200 epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZaaGb01Ohy1",
        "outputId": "10ecfa8d-02b9-405f-ecec-16b129ab96ac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 starting ...\n",
            "complete!\n",
            "Epoch 1 starting ...\n",
            "complete!\n",
            "Epoch 2 starting ...\n",
            "complete!\n",
            "Epoch 3 starting ...\n",
            "complete!\n",
            "Epoch 4 starting ...\n",
            "complete!\n",
            "Epoch 5 starting ...\n",
            "complete!\n",
            "Epoch 6 starting ...\n",
            "complete!\n",
            "Epoch 7 starting ...\n",
            "complete!\n",
            "Epoch 8 starting ...\n",
            "complete!\n",
            "Epoch 9 starting ...\n",
            "complete!\n",
            "Epoch 10 starting ...\n",
            "complete!\n",
            "Epoch 11 starting ...\n",
            "complete!\n",
            "Epoch 12 starting ...\n",
            "complete!\n",
            "Epoch 13 starting ...\n",
            "complete!\n",
            "Epoch 14 starting ...\n",
            "complete!\n",
            "Epoch 15 starting ...\n",
            "complete!\n",
            "Epoch 16 starting ...\n",
            "complete!\n",
            "Epoch 17 starting ...\n",
            "complete!\n",
            "Epoch 18 starting ...\n",
            "complete!\n",
            "Epoch 19 starting ...\n",
            "complete!\n",
            "Epoch 20 starting ...\n",
            "complete!\n",
            "Epoch 21 starting ...\n",
            "complete!\n",
            "Epoch 22 starting ...\n",
            "complete!\n",
            "Epoch 23 starting ...\n",
            "complete!\n",
            "Epoch 24 starting ...\n",
            "complete!\n",
            "Epoch 25 starting ...\n",
            "complete!\n",
            "Epoch 26 starting ...\n",
            "complete!\n",
            "Epoch 27 starting ...\n",
            "complete!\n",
            "Epoch 28 starting ...\n",
            "complete!\n",
            "Epoch 29 starting ...\n",
            "complete!\n",
            "Epoch 30 starting ...\n",
            "complete!\n",
            "Epoch 31 starting ...\n",
            "complete!\n",
            "Epoch 32 starting ...\n",
            "complete!\n",
            "Epoch 33 starting ...\n",
            "complete!\n",
            "Epoch 34 starting ...\n",
            "complete!\n",
            "Epoch 35 starting ...\n",
            "complete!\n",
            "Epoch 36 starting ...\n",
            "complete!\n",
            "Epoch 37 starting ...\n",
            "complete!\n",
            "Epoch 38 starting ...\n",
            "complete!\n",
            "Epoch 39 starting ...\n",
            "complete!\n",
            "Epoch 40 starting ...\n",
            "complete!\n",
            "Epoch 41 starting ...\n",
            "complete!\n",
            "Epoch 42 starting ...\n",
            "complete!\n",
            "Epoch 43 starting ...\n",
            "complete!\n",
            "Epoch 44 starting ...\n",
            "complete!\n",
            "Epoch 45 starting ...\n",
            "complete!\n",
            "Epoch 46 starting ...\n",
            "complete!\n",
            "Epoch 47 starting ...\n",
            "complete!\n",
            "Epoch 48 starting ...\n",
            "complete!\n",
            "Epoch 49 starting ...\n",
            "complete!\n",
            "Epoch 50 starting ...\n",
            "complete!\n",
            "Epoch 51 starting ...\n",
            "complete!\n",
            "Epoch 52 starting ...\n",
            "complete!\n",
            "Epoch 53 starting ...\n",
            "complete!\n",
            "Epoch 54 starting ...\n",
            "complete!\n",
            "Epoch 55 starting ...\n",
            "complete!\n",
            "Epoch 56 starting ...\n",
            "complete!\n",
            "Epoch 57 starting ...\n",
            "complete!\n",
            "Epoch 58 starting ...\n",
            "complete!\n",
            "Epoch 59 starting ...\n",
            "complete!\n",
            "Epoch 60 starting ...\n",
            "complete!\n",
            "Epoch 61 starting ...\n",
            "complete!\n",
            "Epoch 62 starting ...\n",
            "complete!\n",
            "Epoch 63 starting ...\n",
            "complete!\n",
            "Epoch 64 starting ...\n",
            "complete!\n",
            "Epoch 65 starting ...\n",
            "complete!\n",
            "Epoch 66 starting ...\n",
            "complete!\n",
            "Epoch 67 starting ...\n",
            "complete!\n",
            "Epoch 68 starting ...\n",
            "complete!\n",
            "Epoch 69 starting ...\n",
            "complete!\n",
            "Epoch 70 starting ...\n",
            "complete!\n",
            "Epoch 71 starting ...\n",
            "complete!\n",
            "Epoch 72 starting ...\n",
            "complete!\n",
            "Epoch 73 starting ...\n",
            "complete!\n",
            "Epoch 74 starting ...\n",
            "complete!\n",
            "Epoch 75 starting ...\n",
            "complete!\n",
            "Epoch 76 starting ...\n",
            "complete!\n",
            "Epoch 77 starting ...\n",
            "complete!\n",
            "Epoch 78 starting ...\n",
            "complete!\n",
            "Epoch 79 starting ...\n",
            "complete!\n",
            "Epoch 80 starting ...\n",
            "complete!\n",
            "Epoch 81 starting ...\n",
            "complete!\n",
            "Epoch 82 starting ...\n",
            "complete!\n",
            "Epoch 83 starting ...\n",
            "complete!\n",
            "Epoch 84 starting ...\n",
            "complete!\n",
            "Epoch 85 starting ...\n",
            "complete!\n",
            "Epoch 86 starting ...\n",
            "complete!\n",
            "Epoch 87 starting ...\n",
            "complete!\n",
            "Epoch 88 starting ...\n",
            "complete!\n",
            "Epoch 89 starting ...\n",
            "complete!\n",
            "Epoch 90 starting ...\n",
            "complete!\n",
            "Epoch 91 starting ...\n",
            "complete!\n",
            "Epoch 92 starting ...\n",
            "complete!\n",
            "Epoch 93 starting ...\n",
            "complete!\n",
            "Epoch 94 starting ...\n",
            "complete!\n",
            "Epoch 95 starting ...\n",
            "complete!\n",
            "Epoch 96 starting ...\n",
            "complete!\n",
            "Epoch 97 starting ...\n",
            "complete!\n",
            "Epoch 98 starting ...\n",
            "complete!\n",
            "Epoch 99 starting ...\n",
            "complete!\n",
            "Epoch 100 starting ...\n",
            "complete!\n",
            "Epoch 101 starting ...\n",
            "complete!\n",
            "Epoch 102 starting ...\n",
            "complete!\n",
            "Epoch 103 starting ...\n",
            "complete!\n",
            "Epoch 104 starting ...\n",
            "complete!\n",
            "Epoch 105 starting ...\n",
            "complete!\n",
            "Epoch 106 starting ...\n",
            "complete!\n",
            "Epoch 107 starting ...\n",
            "complete!\n",
            "Epoch 108 starting ...\n",
            "complete!\n",
            "Epoch 109 starting ...\n",
            "complete!\n",
            "Epoch 110 starting ...\n",
            "complete!\n",
            "Epoch 111 starting ...\n",
            "complete!\n",
            "Epoch 112 starting ...\n",
            "complete!\n",
            "Epoch 113 starting ...\n",
            "complete!\n",
            "Epoch 114 starting ...\n",
            "complete!\n",
            "Epoch 115 starting ...\n",
            "complete!\n",
            "Epoch 116 starting ...\n",
            "complete!\n",
            "Epoch 117 starting ...\n",
            "complete!\n",
            "Epoch 118 starting ...\n",
            "complete!\n",
            "Epoch 119 starting ...\n",
            "complete!\n",
            "Epoch 120 starting ...\n",
            "complete!\n",
            "Epoch 121 starting ...\n",
            "complete!\n",
            "Epoch 122 starting ...\n",
            "complete!\n",
            "Epoch 123 starting ...\n",
            "complete!\n",
            "Epoch 124 starting ...\n",
            "complete!\n",
            "Epoch 125 starting ...\n",
            "complete!\n",
            "Epoch 126 starting ...\n",
            "complete!\n",
            "Epoch 127 starting ...\n",
            "complete!\n",
            "Epoch 128 starting ...\n",
            "complete!\n",
            "Epoch 129 starting ...\n",
            "complete!\n",
            "Epoch 130 starting ...\n",
            "complete!\n",
            "Epoch 131 starting ...\n",
            "complete!\n",
            "Epoch 132 starting ...\n",
            "complete!\n",
            "Epoch 133 starting ...\n",
            "complete!\n",
            "Epoch 134 starting ...\n",
            "complete!\n",
            "Epoch 135 starting ...\n",
            "complete!\n",
            "Epoch 136 starting ...\n",
            "complete!\n",
            "Epoch 137 starting ...\n",
            "complete!\n",
            "Epoch 138 starting ...\n",
            "complete!\n",
            "Epoch 139 starting ...\n",
            "complete!\n",
            "Epoch 140 starting ...\n",
            "complete!\n",
            "Epoch 141 starting ...\n",
            "complete!\n",
            "Epoch 142 starting ...\n",
            "complete!\n",
            "Epoch 143 starting ...\n",
            "complete!\n",
            "Epoch 144 starting ...\n",
            "complete!\n",
            "Epoch 145 starting ...\n",
            "complete!\n",
            "Epoch 146 starting ...\n",
            "complete!\n",
            "Epoch 147 starting ...\n",
            "complete!\n",
            "Epoch 148 starting ...\n",
            "complete!\n",
            "Epoch 149 starting ...\n",
            "complete!\n",
            "Epoch 150 starting ...\n",
            "complete!\n",
            "Epoch 151 starting ...\n",
            "complete!\n",
            "Epoch 152 starting ...\n",
            "complete!\n",
            "Epoch 153 starting ...\n",
            "complete!\n",
            "Epoch 154 starting ...\n",
            "complete!\n",
            "Epoch 155 starting ...\n",
            "complete!\n",
            "Epoch 156 starting ...\n",
            "complete!\n",
            "Epoch 157 starting ...\n",
            "complete!\n",
            "Epoch 158 starting ...\n",
            "complete!\n",
            "Epoch 159 starting ...\n",
            "complete!\n",
            "Epoch 160 starting ...\n",
            "complete!\n",
            "Epoch 161 starting ...\n",
            "complete!\n",
            "Epoch 162 starting ...\n",
            "complete!\n",
            "Epoch 163 starting ...\n",
            "complete!\n",
            "Epoch 164 starting ...\n",
            "complete!\n",
            "Epoch 165 starting ...\n",
            "complete!\n",
            "Epoch 166 starting ...\n",
            "complete!\n",
            "Epoch 167 starting ...\n",
            "complete!\n",
            "Epoch 168 starting ...\n",
            "complete!\n",
            "Epoch 169 starting ...\n",
            "complete!\n",
            "Epoch 170 starting ...\n",
            "complete!\n",
            "Epoch 171 starting ...\n",
            "complete!\n",
            "Epoch 172 starting ...\n",
            "complete!\n",
            "Epoch 173 starting ...\n",
            "complete!\n",
            "Epoch 174 starting ...\n",
            "complete!\n",
            "Epoch 175 starting ...\n",
            "complete!\n",
            "Epoch 176 starting ...\n",
            "complete!\n",
            "Epoch 177 starting ...\n",
            "complete!\n",
            "Epoch 178 starting ...\n",
            "complete!\n",
            "Epoch 179 starting ...\n",
            "complete!\n",
            "Epoch 180 starting ...\n",
            "complete!\n",
            "Epoch 181 starting ...\n",
            "complete!\n",
            "Epoch 182 starting ...\n",
            "complete!\n",
            "Epoch 183 starting ...\n",
            "complete!\n",
            "Epoch 184 starting ...\n",
            "complete!\n",
            "Epoch 185 starting ...\n",
            "complete!\n",
            "Epoch 186 starting ...\n",
            "complete!\n",
            "Epoch 187 starting ...\n",
            "complete!\n",
            "Epoch 188 starting ...\n",
            "complete!\n",
            "Epoch 189 starting ...\n",
            "complete!\n",
            "Epoch 190 starting ...\n",
            "complete!\n",
            "Epoch 191 starting ...\n",
            "complete!\n",
            "Epoch 192 starting ...\n",
            "complete!\n",
            "Epoch 193 starting ...\n",
            "complete!\n",
            "Epoch 194 starting ...\n",
            "complete!\n",
            "Epoch 195 starting ...\n",
            "complete!\n",
            "Epoch 196 starting ...\n",
            "complete!\n",
            "Epoch 197 starting ...\n",
            "complete!\n",
            "Epoch 198 starting ...\n",
            "complete!\n",
            "Epoch 199 starting ...\n",
            "complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###pre process\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "def load_images(directory):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.startswith(\"resized\") and (filename.endswith(\".jpg\") or filename.endswith(\".png\")):\n",
        "            image_path = os.path.join(directory, filename)\n",
        "            image = Image.open(image_path)\n",
        "            #image = image.resize((64, 64))  # Resize the image to desired dimensions - no need to resize since we already have separate script that did that\n",
        "            image = img_to_array(image)\n",
        "            images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Specify the directory containing the images\n",
        "image_directory = \"/Users/quincycrittendon/Desktop/ResizedFrames\"\n",
        "\n",
        "# Load images from the directory\n",
        "dataset = load_images(image_directory)\n",
        "\n",
        "# Normalize the pixel values to the range [-1, 1]\n",
        "dataset = (dataset.astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "# Print the shape of the dataset\n",
        "print(\"Dataset shape:\", dataset.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Oe3K7NpWO8Q",
        "outputId": "f960242f-eadc-4d2c-81db-2ccb58a980f1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (0,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def load_custom_images(directory, size=(32,32)):\n",
        "    images = []\n",
        "    # List all files in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        # Construct the full path to the image\n",
        "        path = os.path.join(directory, filename)\n",
        "        # Open the image file\n",
        "        image = Image.open(path)\n",
        "        # Resize image (just in case some are not 32x32)\n",
        "        image = image.resize(size)\n",
        "        # Convert image to numpy array\n",
        "        image_data = np.asarray(image)\n",
        "        # Scale data to the range [-1, 1]\n",
        "        image_data = (image_data - 127.5) / 127.5\n",
        "        images.append(image_data)\n",
        "    # Convert list to a numpy array\n",
        "    images = np.array(images)\n",
        "    return images\n"
      ],
      "metadata": {
        "id": "ZtTo84XB579p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your custom dataset\n",
        "dataset = load_custom_images('/Users/quincycrittendon/Desktop/ResizedFrames')\n",
        "\n",
        "# Verify the shape and data type\n",
        "print('Dataset shape:', dataset.shape)\n",
        "print('Data type:', dataset.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4jiqgiDWjbL",
        "outputId": "d96873de-2113-4cfe-ee22-cc18725e1fe4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (0,)\n",
            "Data type: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose\n",
        "from keras.layers import LeakyReLU, Dropout\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "def define_discriminator(in_shape=(32, 32, 3)):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=in_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(256, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile model\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def define_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "    n_nodes = 256 * 4 * 4\n",
        "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((4, 4, 256)))\n",
        "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(3, (3, 3), activation='tanh', padding='same'))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "rbECfVw68bwy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_gan(generator, discriminator):\n",
        "    # make weights in the discriminator not trainable\n",
        "    discriminator.trainable = False\n",
        "    model = Sequential()\n",
        "    # add generator\n",
        "    model.add(generator)\n",
        "    # add the discriminator\n",
        "    model.add(discriminator)\n",
        "    # compile model\n",
        "    opt = Adam(lr=0.0001, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "KIOdilpfSDmO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def train_gan(g_model, d_model, gan_model, dataset, latent_dim, epochs=2, batch_size=128):\n",
        "    half_batch = batch_size // 2\n",
        "    for epoch in range(epochs):\n",
        "        for batch in range(dataset.shape[0] // batch_size):\n",
        "            # randomly select real samples\n",
        "            idx = np.random.randint(0, dataset.shape[0], half_batch)\n",
        "            real_images = dataset[idx]\n",
        "            real_y = np.ones((half_batch, 1))\n",
        "            # generate fake images\n",
        "            noise = np.random.randn(latent_dim * half_batch).reshape(half_batch, latent_dim)\n",
        "            fake_images = g_model.predict(noise)\n",
        "            fake_y = np.zeros((half_batch, 1))\n",
        "            # update discriminator model\n",
        "            d_model.train_on_batch(real_images, real_y)\n",
        "            d_model.train_on_batch(fake_images, fake_y)\n",
        "            # prepare points in latent space as input for the generator\n",
        "            x_gan = np.random.randn(latent_dim * batch_size).reshape(batch_size, latent_dim)\n",
        "            y_gan = np.ones((batch_size, 1))\n",
        "            # update the generator via the discriminator's error\n",
        "            gan_model.train_on_batch(x_gan, y_gan)\n",
        "\n",
        "            # Plot a generated image after every batch\n",
        "            if batch % 1 == 0:\n",
        "                show_generated_image(g_model, latent_dim)\n",
        "\n",
        "def show_generated_image(g_model, latent_dim):\n",
        "    noise = np.random.randn(latent_dim).reshape(1, latent_dim)\n",
        "    image = g_model.predict(noise)[0]\n",
        "    image = (image + 1) / 2.0  # Rescale to [0, 1]\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "7_qM766_SLe5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "latent_dim = 100  # Dimensionality of the latent space\n",
        "\n",
        "# Define models\n",
        "discriminator = define_discriminator()\n",
        "generator = define_generator(latent_dim)\n",
        "gan_model = define_gan(generator, discriminator)\n",
        "\n",
        "# Load dataset (assuming this is already done)\n",
        "# dataset = load_custom_images('path/to/EclipseFrames')\n",
        "\n",
        "# Train the GAN\n",
        "train_gan(generator, discriminator, gan_model, dataset, latent_dim, epochs=100, batch_size=128)\n"
      ],
      "metadata": {
        "id": "aAPGZ1uM85br"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}